# Day 9: SAC Baseline

## ğŸ¯ Learning Objectives
- Implement Soft Actor-Critic (SAC) algorithm
- Understand entropy regularization in RL
- Master action squashing for continuous control
- Train on dm_control environments

## ğŸ“š Background
SAC is one of the most successful off-policy RL algorithms for continuous control. It uses entropy regularization to encourage exploration and learns both a policy and Q-functions simultaneously.

## ğŸ› ï¸ Coding Task

### Setup
```
Day09-SAC-Baseline/
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sac.py              # SAC implementation
â”‚   â”œâ”€â”€ networks.py         # Policy and Q-network architectures
â”‚   â””â”€â”€ losses.py           # SAC loss functions
â”œâ”€â”€ environments/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dm_control_wrapper.py
â”‚   â””â”€â”€ env_factory.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_sac.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ hyperparameter_tuning.py
â”œâ”€â”€ demos/
â”‚   â”œâ”€â”€ cheetah_run.py
â”‚   â”œâ”€â”€ finger_spin.py
â”‚   â””â”€â”€ learning_curves.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ sac_config.yaml
â””â”€â”€ requirements.txt
```

### Implementation Requirements

1. **SAC Algorithm** (`algorithms/sac.py`):
   - Soft policy gradient
   - Soft Q-learning updates
   - Automatic entropy tuning
   - Target network updates

2. **Network Architectures** (`algorithms/networks.py`):
   - Gaussian policy network
   - Twin Q-networks
   - Action squashing (tanh)
   - Log probability computation

3. **Loss Functions** (`algorithms/losses.py`):
   - Policy loss with entropy bonus
   - Q-function loss (MSE)
   - Entropy loss for auto-tuning
   - Gradient clipping

4. **dm_control Integration** (`environments/dm_control_wrapper.py`):
   - Wrap dm_control environments
   - Handle observation preprocessing
   - Action space normalization
   - Episode termination logic

### Key Features to Implement

- **Entropy Regularization**: Automatic temperature tuning
- **Twin Q-Networks**: Reduce overestimation bias
- **Action Squashing**: Handle bounded action spaces
- **Target Networks**: Stable learning
- **Gradient Clipping**: Prevent exploding gradients

## ğŸ® Demo Task
Train SAC on two dm_control tasks:

### Task 1: Cheetah Run
- Environment: `dm_control.cheetah.run`
- Goal: Maximize forward velocity
- Success: Average velocity > 8 m/s

### Task 2: Finger Spin
- Environment: `dm_control.finger.spin`
- Goal: Keep finger spinning
- Success: Average angular velocity > 5 rad/s

For each task:
1. Train for 1M steps
2. Log learning curves (return, Q-loss, policy loss)
3. Generate evaluation videos
4. Compare with random policy baseline

## ğŸ“Š Success Criteria
- [ ] SAC implementation matches paper
- [ ] Cheetah reaches >8 m/s average velocity
- [ ] Finger spin maintains >5 rad/s
- [ ] Learning curves show clear improvement
- [ ] Videos show smooth, stable behavior

## ğŸ”§ Dependencies
```bash
pip install torch torchvision
pip install dm_control
pip install gymnasium
pip install wandb tensorboard
```

## ğŸ“ Mathematical Background

### SAC Objective
```
J(Ï€) = E[âˆ‘_{t=0}^T Î³^t (r_t + Î± H(Ï€(Â·|s_t)))]
```

Where:
- H(Ï€(Â·|s_t)) is the entropy of the policy
- Î± is the temperature parameter
- Î³ is the discount factor

### Policy Loss
```
L_Ï€ = E[Î± log Ï€(a_t|s_t) - Q_Î¸(s_t, a_t)]
```

### Q-Function Loss
```
L_Q = E[(Q_Î¸(s_t, a_t) - (r_t + Î³ Q_Î¸'(s_{t+1}, a_{t+1}) - Î± log Ï€(a_{t+1}|s_{t+1})))^2]
```

### Automatic Entropy Tuning
```
L_Î± = E[-Î± log Ï€(a_t|s_t) - Î± H_target]
```

## ğŸ“ Deliverables
- Complete SAC implementation
- Training scripts for both tasks
- Learning curve plots
- Evaluation videos
- Performance comparison report

## ğŸš€ Next Steps
Tomorrow we'll implement TD3 with exploration techniques!
